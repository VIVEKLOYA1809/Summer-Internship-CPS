{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize, special\n",
    "\n",
    "F = np.array([[1.0000, 0.1000],\n",
    "              [0, 1.0000]])\n",
    "\n",
    "G = np.array([[0.0050],\n",
    "              [0.1000]])\n",
    "\n",
    "C = np.array([[1, 0]])\n",
    "\n",
    "D = 0\n",
    "Gain = np.array([16.0302, 5.6622])\n",
    "\n",
    "L = np.array([[0.9902],\n",
    "              [0.9892]])\n",
    "safex = [26,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=14\n",
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "X = np.zeros((2, input_len + 1))\n",
    "Xe = np.zeros((2, input_len + 1))\n",
    "e = np.zeros((2, input_len))\n",
    "U = np.zeros((1,input_len))\n",
    "Y = np.zeros((1,input_len))\n",
    "r = np.zeros((1,input_len))\n",
    "z = np.zeros((1,input_len))\n",
    "S = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])      \n",
    "n2 = np.random.normal(0,0.001, [2, input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(input_len):\n",
    "    e[:, i] = X[:, i] - Xe[:, i]\n",
    "    U[:,i] = -(Gain @ Xe[:, i])\n",
    "    X[:, i+1] = F @ X[:, i] + G @ U[:, i] + n2[:, i]\n",
    "    Y[:,i] = C @ X[:, i] + n1[:,i]\n",
    "    r[:,i] = Y[:,i] - C @ Xe[:, i]\n",
    "    Xe[:, i+1] = F @ Xe[:, i] + G @ U[:,i] + L @ r[:,i]\n",
    "covn_r=np.cov(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attackK= [0, 0.04155838 , 0.63838859 , 1.75426837 ,2.44935936 , 3.6651983 , 5.16561739 , 6.95368618 , 9.11716362 ,11.67267658 ,14.29706254, 17.41085821  ,0.       ,   0.    ]\n",
    "k=len(attackK)\n",
    "attack2K = [-30.971675   ,-30.971675  ,  -1.11684483 ,-30.90514398, -28.29714574 , -28.61888554, -28.91105434, -29.17567371, -29.41469648, -29.21724115 , -28.49741602 , 30.971675,     0.     ,      0. ]\n",
    "k1=len(attack2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "Xk = np.zeros((2, input_len + 1))\n",
    "Xek = np.zeros((2, input_len + 1))\n",
    "ek = np.zeros((2, input_len))\n",
    "Uk = np.zeros((1,input_len))\n",
    "Uak = np.zeros((1,input_len))\n",
    "Yk = np.zeros((1,input_len))\n",
    "rk = np.zeros((1,input_len))\n",
    "covn_eK = np.zeros((2, 2))\n",
    "covn_rK = np.zeros((2, 2))\n",
    "zk = np.zeros((1,input_len))\n",
    "Sk = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])      \n",
    "n2 = np.random.normal(0,0.001, [2,input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuek=[]\n",
    "mzk=[]\n",
    "bias=4\n",
    "for i in range(input_len):\n",
    "    ek[:, i] = Xk[:, i] - Xek[:, i]\n",
    "    Uk[:,i] = -(Gain @ Xek[:, i])\n",
    "    Uak[:,i] = -(Gain @ Xek[:, i])+attack2K[i]\n",
    "    Xk[:, i+1] = F @ Xk[:, i] + G @ Uak[:, i] + n2[:, i]\n",
    "    Yk[:,i] = C @ Xk[:, i] + n1[:,i] + attackK[i]\n",
    "    rk[:,i] = Yk[:,i] - C @ Xek[:, i]\n",
    "    Xek[:, i+1] = F @ Xek[:, i] + G @ Uk[:,i] + L @ rk[:,i]\n",
    "    zk[:,i] = rk[:,i].T * covn_r * rk[:,i]\n",
    "    residuek.append(rk[:,i])\n",
    "    mzk.append(zk[:,i])\n",
    "    Sk[i] = max((Sk[i-1] + zk[:,i] - bias), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.mean(np.array(mzk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def discrete_frame(Sk,N,th):\n",
    "        delSi = 2*th/(2*N-1)\n",
    "        Ek=np.zeros((len(Sk)))\n",
    "        for i in range(len(Sk)):\n",
    "            if Sk[i]<delSi[i]/2:\n",
    "                Ek[i]=0\n",
    "            elif Sk[i]>=th[i]:\n",
    "                Ek[i]=N\n",
    "            else :\n",
    "                Ek[i]=math.ceil(math.floor(Sk[i]/(delSi[i]/2))/2)\n",
    "        return Ek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuek=np.absolute(np.array(residuek))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mean=np.array([])\n",
    "for i in range(len(residuek)):\n",
    "    sums=0\n",
    "    for j in range(i+1):\n",
    "        sums+=residuek[j]\n",
    "    mean=sums/(j+1)\n",
    "    r_mean=np.append(r_mean,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(r_mean>0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import erf\n",
    "import scipy.special as special\n",
    "from scipy import special, optimize\n",
    "def markov_transition(N, th, bias):\n",
    "    temp = np.zeros((N+1, N+1))\n",
    "    for i in range(1,N+1):\n",
    "        for j in range(1,N+1):\n",
    "            if j == 1 & i <= N:\n",
    "                temp[i, j] = Tx(1-i, th, bias, N)\n",
    "            elif j > 1 & j < N+1 & i <= N:\n",
    "                temp[i, j] = Px(j-i, th, bias, N)\n",
    "            elif j == N+1 & i<=N:\n",
    "                temp[i, j] = 1 - Tx(N-i, th, bias, N)\n",
    "            elif i == N+1 & j < N+1:\n",
    "                temp[i, j] = 0\n",
    "            else:\n",
    "                temp[i, j] = 1\n",
    "    P = temp\n",
    "    return P\n",
    "\n",
    "def Px(x, th, bias, N):\n",
    "    delSi = 2*th/(2*N-1)\n",
    "    z1 = np.max(x*delSi + 0.5*delSi + bias, 0)\n",
    "    p1 = special.gammainc(m/2, z1/2)\n",
    "    z2 = np.max(x*delSi - 0.5*delSi + bias, 0)\n",
    "    p2 = special.gammainc(m/2, z2/2)\n",
    "    p = p1-p2\n",
    "    return p\n",
    "def Tx(x, th, bias, N):\n",
    "    delSi = 2*th/(2*N-1)\n",
    "    z = np.max(x*delSi + 0.5*delSi + bias, 0)\n",
    "    t = special.gammainc(m/2, z/2)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#     def objective(params,m,N,E):\n",
    "#         th, bias = params\n",
    "#         x=np.ones((len(E),1))*(N-1)-E\n",
    "#         delSi = 2 * th / (2 * N - 1)\n",
    "#         z = np.maximum(x * (delSi*np.ones((1,len(E)))) + 0.5 * delSi + bias, 0)\n",
    "#         t = special.gammainc(m / 2, z / 2)-1\n",
    "#         return -t  # Negate t since we want to maximize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization():\n",
    "    \n",
    "    def __init__(self,params,m,N,Ek):\n",
    "        self.params=params.T\n",
    "        self.threshold=params[:,0]\n",
    "        self.bias=params[:,1]\n",
    "        self.m=m\n",
    "        self.N=N\n",
    "        self.Ek=Ek\n",
    "\n",
    "    def gradient(self):\n",
    "        x=np.zeros((len(self.Ek)))\n",
    "        z=np.zeros((len(self.Ek)))\n",
    "        t=np.zeros((len(self.Ek)))\n",
    "        dt_dth=np.zeros((len(self.Ek)))\n",
    "        dt_dbias=np.zeros((len(self.Ek)))\n",
    "        for i in range(len(self.Ek)):\n",
    "            x[i]=self.N-1-self.Ek[i]\n",
    "            delSi = 2 * self.threshold[i] / (2 * self.N - 1)\n",
    "            z[i] = np.maximum(x[i] * delSi + 0.5 * delSi + self.bias[i], 0)\n",
    "            t[i] = 1 - special.gammainc(m / 2, z[i]/ 2)\n",
    "            dt_dth[i] = 0.5 * z[i] * np.exp(-z[i] / 2) * x[i] / (2 *self.N - 1)\n",
    "            dt_dbias[i] = 0.5 * z[i] * np.exp(-z[i] / 2)\n",
    "        return np.array([-dt_dth, -dt_dbias])\n",
    "\n",
    "    def adam_optimizer(self,learning_rate=0.8, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=2):\n",
    "        p = np.zeros_like(self.params)\n",
    "        v = np.zeros_like(self.params)\n",
    "        initial_values=self.params\n",
    "\n",
    "        for t in range(1,num_iterations+1):\n",
    "            gradient = self.gradient()\n",
    "            p = beta1 * p + (1 - beta1) * gradient\n",
    "            v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
    "            p_hat = p / (1 - beta1 ** t)\n",
    "            v_hat = v / (1 - beta2 ** t)\n",
    "            initial_values += learning_rate * p_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        return initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RLEnvironment():\n",
    "    def __init__(self, num_states,num_action,cusum,residual,action,pos,N):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_action\n",
    "        self.cusum = cusum\n",
    "        self.residual = residual\n",
    "        self.threshold = action[:,0]\n",
    "        self.absorb=N\n",
    "        self.current_state = None\n",
    "        \n",
    "    def discrete_frame(self):\n",
    "        delSi = 2*self.threshold/(2*self.absorb-1)\n",
    "        Ek=np.zeros((len(self.cusum)))\n",
    "        for i in range(len(self.cusum)):\n",
    "            if self.cusum[i]<delSi[i]/2:\n",
    "                Ek[i]=0\n",
    "            elif self.cusum[i]>=self.threshold[i]:\n",
    "                Ek[i]=self.absorb\n",
    "            else :\n",
    "                Ek[i]=math.ceil(math.floor(self.cusum[i]/(delSi[i]/2))/2)\n",
    "        return Ek.reshape(1,-1)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_state = self.discrete_frame()\n",
    "        return self.current_state\n",
    "    \n",
    "    def calculate_reward(self,i):\n",
    "        # Calculate reward based on attack detection condition\n",
    "        if self.residual[i]>0.01 and self.cusum[i]> self.threshold[i]:\n",
    "            return 10\n",
    "        elif self.residual[i]<0.01 and self.cusum[i]<self.threshold[i]:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def step(self,pos):\n",
    "        \n",
    "        next_state = self.discrete_frame()\n",
    "        \n",
    "        reward = self.calculate_reward(pos)\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        done = (self.current_state == self.absorb)\n",
    "        \n",
    "        return next_state, reward, done,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define actor and critic neural network architectures\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.output_layer = nn.Linear(300, action_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = F.relu(self.fc1(inputs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        actions = torch.tanh(self.output_layer(x))\n",
    "        return actions\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.output_layer = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action[:,0].reshape(1,-1)], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q_value = self.output_layer(x)\n",
    "        return q_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "threshold = 6\n",
    "pos=0\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACTOR = 0.0001\n",
    "LR_CRITIC = 0.001\n",
    "N=10\n",
    "threshold=np.ones((len(Sk),1))*6\n",
    "bias=np.ones((len(Sk),1))*4\n",
    "action = np.concatenate((threshold,bias),axis=1)\n",
    "num_action=len(action)\n",
    "Ek=discrete_frame(Sk,N,action[:,0])\n",
    "optimizer=Optimization(action,m,N,Ek)\n",
    "env = RLEnvironment(len(Sk),num_action,Sk,r_mean,action,pos,N)\n",
    "\n",
    "# Initialize actor, critic, target actor, and target critic networks\n",
    "state_dim = env.num_states\n",
    "action_dim = env.num_actions\n",
    "\n",
    "# Create actor and critic networks\n",
    "actor = ActorNetwork(state_dim, action_dim)\n",
    "critic = CriticNetwork(state_dim, action_dim)\n",
    "\n",
    "# Create target actor and critic networks and initialize them with main network weights\n",
    "target_actor = copy.deepcopy(actor)\n",
    "target_critic = copy.deepcopy(critic)\n",
    "\n",
    "# Define optimizer for actor and critic networks\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=LR_ACTOR)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=LR_CRITIC)\n",
    "\n",
    "\n",
    "# Other components like replay buffer, training loop, exploration noise, etc.\n",
    "# should be implemented and integrated into this code as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define other hyperparameters and initialization steps\n",
    "learning_rate_actor = 0.001\n",
    "learning_rate_critic = 0.001\n",
    "learning_rate_threshold = 0.01  # Learning rate for the threshold optimizer\n",
    "# batch_size = 64\n",
    "buffer_size = 10000\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "num_episodes = 10\n",
    "N=10\n",
    "batch_size=1\n",
    "for episode in range(num_episodes):\n",
    "    states = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    for i in range(len(Sk)):\n",
    "        # Select action using actor network\n",
    "        pos=i\n",
    "        actions = optimizer.adam_optimizer()\n",
    "        threshold=actions[0]\n",
    "        bias=actions[1]\n",
    "        \n",
    "        \n",
    "        # Perform action in the environment\n",
    "        next_states, rewards, dones = env.step(pos)\n",
    "        print(states)\n",
    "        print(actions)\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Update critic network\n",
    "        q_values = critic(states, actions)\n",
    "        next_actions = target_actor(next_states)\n",
    "        next_q_values = target_critic(next_states, next_actions)\n",
    "        target_q_values = rewards + gamma * (1 - dones) * next_q_values\n",
    "        critic_loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Update actor network\n",
    "        actor_loss = -critic(states, actor(states)).mean()\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        for i in range(1,len(mzk)):\n",
    "            Sk[i]=max(Sk[i-1]+mzk[i]-bias,0)\n",
    "        \n",
    "        states = next_states\n",
    "        episode_reward += rewards\n",
    "\n",
    "\n",
    "        # Update target networks using soft updates\n",
    "        for target_param, param in zip(target_actor.parameters(), actor.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        for target_param, param in zip(target_critic.parameters(), critic.parameters()):\n",
    "            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    print(f\"Episode {episode+1} - Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the trained actor network for generating actions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
