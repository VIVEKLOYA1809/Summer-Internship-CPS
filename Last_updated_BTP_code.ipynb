{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb03e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4cf3135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize, special\n",
    "\n",
    "F = np.array([[1.0000, 0.1000],\n",
    "              [0, 1.0000]])\n",
    "\n",
    "G = np.array([[0.0050],\n",
    "              [0.1000]])\n",
    "\n",
    "C = np.array([[1, 0]])\n",
    "\n",
    "D = 0\n",
    "Gain = np.array([16.0302, 5.6622])\n",
    "\n",
    "L = np.array([[0.9902],\n",
    "              [0.9892]])\n",
    "safex = [26,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39da914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import optimize, special\n",
    "\n",
    "F = np.array([[1.0000, 0.1000],\n",
    "              [0, 1.0000]])\n",
    "\n",
    "G = np.array([[0.0050],\n",
    "              [0.1000]])\n",
    "\n",
    "C = np.array([[1, 0]])\n",
    "\n",
    "D = 0\n",
    "Gain = np.array([16.0302, 5.6622])\n",
    "\n",
    "L = np.array([[0.9902],\n",
    "              [0.9892]])\n",
    "safex = [26,31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045d7eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=14\n",
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "X = np.zeros((2, input_len + 1))\n",
    "Xe = np.zeros((2, input_len + 1))\n",
    "e = np.zeros((2, input_len))\n",
    "U = np.zeros((1,input_len))\n",
    "Y = np.zeros((1,input_len))\n",
    "r = np.zeros((1,input_len))\n",
    "z = np.zeros((1,input_len))\n",
    "S = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])     \n",
    "n2 = np.random.normal(0,0.001, [2, input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8a02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(input_len):\n",
    "    e[:, i] = X[:, i] - Xe[:, i]\n",
    "    U[:,i] = -(Gain @ Xe[:, i])\n",
    "    X[:, i+1] = F @ X[:, i] + G @ U[:, i] + n2[:, i]\n",
    "    Y[:,i] = C @ X[:, i] + n1[:,i]\n",
    "    r[:,i] = Y[:,i] - C @ Xe[:, i]\n",
    "    Xe[:, i+1] = F @ Xe[:, i] + G @ U[:,i] + L @ r[:,i]\n",
    "covn_r=np.cov(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f59f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attackK= [0, 0.04155838 , 0.63838859 , 1.75426837 ,2.44935936 , 3.6651983 , 5.16561739 , 6.95368618 , 9.11716362 ,11.67267658 ,14.29706254, 17.41085821  ,0.       ,   0.    ]\n",
    "k=len(attackK)\n",
    "attack2K = [-30.971675   ,-30.971675  ,  -1.11684483 ,-30.90514398, -28.29714574 , -28.61888554, -28.91105434, -29.17567371, -29.41469648, -29.21724115 , -28.49741602 , 30.971675,     0.     ,      0. ]\n",
    "k1=len(attack2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59815bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.random.normal(0, 1, k)\n",
    "input_len=len(inputs)\n",
    "Xk = np.zeros((2, input_len + 1))\n",
    "Xek = np.zeros((2, input_len + 1))\n",
    "ek = np.zeros((2, input_len))\n",
    "Uk = np.zeros((1,input_len))\n",
    "Uak = np.zeros((1,input_len))\n",
    "Yk = np.zeros((1,input_len))\n",
    "rk = np.zeros((1,input_len))\n",
    "covn_eK = np.zeros((2, 2))\n",
    "covn_rK = np.zeros((2, 2))\n",
    "zk = np.zeros((1,input_len))\n",
    "Sk = np.zeros((input_len))\n",
    "n1 = np.random.normal(0,0.01, [1, input_len])      \n",
    "n2 = np.random.normal(0,0.001, [2,input_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86a212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuek=[]\n",
    "mzk=[]\n",
    "bias=4\n",
    "for i in range(input_len):\n",
    "    ek[:, i] = Xk[:, i] - Xek[:, i]\n",
    "    Uk[:,i] = -(Gain @ Xek[:, i])\n",
    "    Uak[:,i] = -(Gain @ Xek[:, i])+attack2K[i]\n",
    "    Xk[:, i+1] = F @ Xk[:, i] + G @ Uak[:, i] + n2[:, i]\n",
    "    Yk[:,i] = C @ Xk[:, i] + n1[:,i] + attackK[i]\n",
    "    rk[:,i] = Yk[:,i] - C @ Xek[:, i]\n",
    "    Xek[:, i+1] = F @ Xek[:, i] + G @ Uk[:,i] + L @ rk[:,i]\n",
    "    zk[:,i] = rk[:,i].T * covn_r * rk[:,i]\n",
    "    residuek.append(rk[:,i])\n",
    "    mzk.append(zk[:,i])\n",
    "    Sk[i] = Sk[i-1] + zk[:,i] - bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a036d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=np.mean(np.array(mzk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecb06349",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_mean=np.array([])\n",
    "for i in range(len(residuek)):\n",
    "    sums=0\n",
    "    for j in range(i+1):\n",
    "        sums+=residuek[j]\n",
    "    mean=sums/(j+1)\n",
    "    r_mean=np.append(r_mean,mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39081c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd44f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action,next_state, reward, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action,next_state, reward, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward,  done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward,  done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9b0cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = low_bound + (action + 1.0) * 0.5 * (upper_bound - low_bound)\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low_bound   = self.action_space.low\n",
    "        upper_bound = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low_bound) / (upper_bound - low_bound) - 1\n",
    "        action = np.clip(action, low_bound, upper_bound)\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95770396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "    \n",
    "#https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fba81250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot(frame_idx, rewards):\n",
    "#     clear_output(True)\n",
    "#     plt.figure(figsize=(20,5))\n",
    "#     plt.subplot(131)\n",
    "#     plt.title('frame %s. reward: %s' % (frame_idx, rewards[-1]))\n",
    "#     plt.plot(rewards)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb6004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Optimization():\n",
    "    \n",
    "#     def __init__(self,params,m,N,Ek):\n",
    "#         self.params=params.T\n",
    "#         self.threshold=params[:,0]\n",
    "#         self.bias=params[:,1]\n",
    "#         self.m=m\n",
    "#         self.N=N\n",
    "#         self.Ek=Ek\n",
    "\n",
    "#     def gradient(self):\n",
    "#         x=np.zeros((len(self.Ek)))\n",
    "#         z=np.zeros((len(self.Ek)))\n",
    "#         t=np.zeros((len(self.Ek)))\n",
    "#         dt_dth=np.zeros((len(self.Ek)))\n",
    "#         dt_dbias=np.zeros((len(self.Ek)))\n",
    "#         for i in range(len(self.Ek)):\n",
    "#             x[i]=self.N-1-self.Ek[i]\n",
    "#             delSi = 2 * self.threshold[i] / (2 * self.N - 1)\n",
    "#             z[i] = np.maximum(x[i] * delSi + 0.5 * delSi + self.bias[i], 0)\n",
    "#             t[i] = 1 - special.gammainc(m / 2, z[i]/ 2)\n",
    "#             dt_dth[i] = 0.5 * z[i] * np.exp(-z[i] / 2) * x[i] / (2 *self.N - 1)\n",
    "#             dt_dbias[i] = 0.5 * z[i] * np.exp(-z[i] / 2)\n",
    "#         return np.array([-dt_dth, -dt_dbias])\n",
    "\n",
    "#     def adam_optimizer(self,learning_rate=0.8, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=2):\n",
    "#         p = np.zeros_like(self.params)\n",
    "#         v = np.zeros_like(self.params)\n",
    "#         initial_values=self.params\n",
    "\n",
    "#         for t in range(1,num_iterations+1):\n",
    "#             gradient = self.gradient()\n",
    "#             p = beta1 * p + (1 - beta1) * gradient\n",
    "#             v = beta2 * v + (1 - beta2) * (gradient ** 2)\n",
    "#             p_hat = p / (1 - beta1 ** t)\n",
    "#             v_hat = v / (1 - beta2 ** t)\n",
    "#             initial_values += learning_rate * p_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "#         return initial_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fbd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action],1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(5, 6)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.tanh(self.linear3(x))\n",
    "        return x\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state  = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        action = self.forward(state)\n",
    "        return action.detach().cpu().numpy()[0, 0],action.detach().cpu().numpy()[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70683f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "import scipy.special as special\n",
    "from scipy import special, optimize\n",
    "def ddpg_update(batch_size,Ek,m,\n",
    "           gamma = 0.99,\n",
    "           min_value=-np.inf,\n",
    "           max_value=np.inf,\n",
    "           soft_tau=1e-2):\n",
    "    \n",
    "    state, action,next_state, reward, done = replay_buffer.sample(batch_size)\n",
    "    state      = torch.FloatTensor(state).to(device)\n",
    "    next_state = torch.FloatTensor(next_state).to(device)\n",
    "    action     = torch.FloatTensor(action).to(device)\n",
    "    reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
    "    done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "    policy_loss = value_net(state, policy_net(state))\n",
    "    policy_loss = -policy_loss.mean()\n",
    "            \n",
    "    x=N-1-Ek\n",
    "\n",
    "    next_action    = target_policy_net(next_state)\n",
    "    th=next_action[0]\n",
    "    bias=next_action[1]\n",
    "    delSi = 2 * th / (2 * N - 1)\n",
    "    z = np.maximum(x * delSi + 0.5 * delSi + bias, 0)\n",
    "    target_value   = target_value_net(next_state, next_action.detach())\n",
    "    expected_value = reward + 1 - special.gammainc(m / 2, z/ 2)\n",
    "    expected_value = torch.clamp(expected_value, min_value, max_value)\n",
    "\n",
    "    value = value_net(state, action)\n",
    "    value_loss = value_criterion(value, expected_value.detach())\n",
    "\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    policy_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "    value_optimizer.zero_grad()\n",
    "    value_loss.backward()\n",
    "    value_optimizer.step()\n",
    "\n",
    "    for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "    for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb8756b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RLEnvironment():\n",
    "    def __init__(self, num_states,num_action,cusum,residual,mzk,N):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_action\n",
    "        self.cusum = cusum\n",
    "        self.residual = residual\n",
    "        self.absorb=N\n",
    "        self.mzk=mzk\n",
    "        self.current_state = None\n",
    "        \n",
    "    def discrete_frame(self,threshold):\n",
    "        delSi = 2*threshold/(2*self.absorb-1)\n",
    "        Ek=np.zeros((len(self.cusum)))\n",
    "        for i in range(len(self.cusum)):\n",
    "            if self.cusum[i]<delSi[i]/2:\n",
    "                Ek[i]=0\n",
    "            elif self.cusum[i]>=threshold[i]:\n",
    "                Ek[i]=self.absorb\n",
    "            else :\n",
    "                Ek[i]=math.ceil(math.floor(self.cusum[i]/(delSi[i]/2))/2)\n",
    "        return Ek.reshape(1,-1)\n",
    "        \n",
    "    def reset(self,bias):\n",
    "        self.cusum = np.zeros((input_len))\n",
    "        for i in range(len(self.cusum)):\n",
    "            if i>0:\n",
    "                self.cusum[i]=self.cusum[i-1]+self.mzk[i]-bias[i]\n",
    "            else:\n",
    "                self.cusum[i]=self.mzk[i]-bias[i]\n",
    "        return self.cusum\n",
    "    \n",
    "    def calculate_reward(self,action):\n",
    "        # Calculate reward based on attack detection condition\n",
    "        if self.residual[i]>0.01 and self.cusum[i] > action[0]:\n",
    "            return 10\n",
    "        elif self.residual[i]<0.01 and self.cusum[i] < action[0]:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def step(self,action,i):\n",
    "        b=action[1]\n",
    "        \n",
    "        next_state = self.cusum[i-1]+self.mzk[i]-b\n",
    "        \n",
    "        reward = self.calculate_reward(action)\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        done = all(self.current_state == self.absorb)\n",
    "        \n",
    "        return  next_state, reward, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "687c4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = NormalizedActions(gym.make(\"Pendulum-v0\"))\n",
    "# ou_noise = OUNoise(env.action_space)\n",
    "num_action=2\n",
    "num_states=2\n",
    "N=10\n",
    "env = RLEnvironment(num_states,num_action,Sk,r_mean,mzk,N)\n",
    "state_dim  = env.num_states\n",
    "action_dim = env.num_actions\n",
    "hidden_dim = 256\n",
    "\n",
    "value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "target_value_net  = ValueNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "target_policy_net = PolicyNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "for target_param, param in zip(target_value_net.parameters(), value_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "\n",
    "for target_param, param in zip(target_policy_net.parameters(), policy_net.parameters()):\n",
    "    target_param.data.copy_(param.data)\n",
    "    \n",
    "    \n",
    "value_lr  = 1e-3\n",
    "policy_lr = 1e-4\n",
    "\n",
    "value_optimizer  = optim.Adam(value_net.parameters(),  lr=value_lr)\n",
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=policy_lr)\n",
    "\n",
    "value_criterion = nn.MSELoss()\n",
    "\n",
    "replay_buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(replay_buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b61839d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames  = 12\n",
    "# max_steps   = 5\n",
    "frame_idx   = 0\n",
    "rewards     = []\n",
    "batch_size  = 128\n",
    "th=np.ones((len(Sk),1))*6\n",
    "bias=np.ones((len(Sk),1))*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bbcbc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "while frame_idx < max_frames:\n",
    "    state = env.reset(bias)\n",
    "    max_steps=len(state)\n",
    "#     ou_noise.reset()\n",
    "    episode_reward = 0\n",
    "    E=env.discrete_frame(th)\n",
    "    for step in range(1,max_steps):\n",
    "        th[step],bias[step] = policy_net.get_action(np.array([state[step],state[step]]))\n",
    "#         action = ou_noise.get_action(action, step)\n",
    "        action=[th[step],bias[step]]\n",
    "        next_state,reward, done = env.step(action,step)\n",
    "        replay_buffer.push(state[step], action,next_state, reward, done)\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            ddpg_update(batch_size,E[step],m)\n",
    "        episode_reward += reward\n",
    "        frame_idx += 1\n",
    "        \n",
    "#         if frame_idx % max(1000, max_steps + 1) == 0:\n",
    "#             plot(frame_idx, rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bda720c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step=1\n",
    "policy_net.get_action(np.array([state[step],state[step]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ec9dc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.]), array([1.])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "456f3cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd343e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ba2dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
